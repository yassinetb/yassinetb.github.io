<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The North Star of My PhD (Part 2) | Yassine Taoudi-Benchekroun </title> <meta name="author" content="Yassine Taoudi-Benchekroun"> <meta name="description" content="PhD Candidate at the Institute of Neuroinformatics, ETH Zurich."> <meta name="keywords" content="Ph.D., ETH Zurich, Machine Learning, Neuroscience, Symbolic-AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/initials_logo_cropped.jpg?994b4169d4373dd59f0fdf180cfbc58a"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.yassine.fyi/blog/phd-north-star-continued/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The North Star of My PhD (Part 2)",
            "description": "",
            "published": "November 26, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yassine Taoudi-Benchekroun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/reads-2025/"> </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Science </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/projects/">Open Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/past_projects/">Past Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/work-with-me/">Work with me</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Activities </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/advising/">Advising</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/talks/">Talks</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Writing </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/blog/">Blog</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/vignettes/">Vignettes</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Random </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/internet-favourites/">Internet Favourites</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/bookshelf/">Bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/reads/">Stuff I Read &amp; Watch</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/to-do-list/">To-Do-List</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The North Star of My PhD (Part 2)</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#system-1-and-system-2-a-paradigm-for-progress-in-ai-research">System 1 and System 2 - A Paradigm for Progress in AI Research</a> </div> <div> <a href="#can-neural-networks-do-system-1">Can Neural Networks do System 1?</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#neural-networks-are-quintessential-system-1">Neural Networks are quintessential System 1</a> </li> <li> <a href="#foundation-models-as-generalist-system-1-models">Foundation Models as generalist System 1 models</a> </li> <li> <a href="#what-s-missing-for-good-system-1-thinking">What's missing for good system 1 thinking?</a> </li> </ul> <div> <a href="#can-neural-networks-do-system-2">Can Neural Networks do System 2?</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#traditional-system-2-planners-and-search">Traditional System 2 - Planners And Search</a> </li> <li> <a href="#what-s-missing">What's Missing</a> </li> </ul> <div> <a href="#how-do-we-connect-system-1-and-system-2">How do we connect System 1 and System 2?</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#chain-of-thought-and-test-time-training">Chain-Of-Thought and Test-Time-Training</a> </li> <li> <a href="#neuro-symbolic-approaches">Neuro-Symbolic Approaches</a> </li> <li> <a href="#what-does-neuroscience-tell-us-about-system-1-and-system-2-thinking">What does Neuroscience tell us about System 1 and System 2 thinking?</a> </li> </ul> </nav> </d-contents> <p>You may have heard of System 1 &amp; System 2 thinking paradigms for AI - inspired from Cognitive Science theory. Greg Brockman (OpenAI’s CEO) Tweeted this a couple of weeks ago, upon release of OpenAI’s latest model “OpenAI o1”. So what’s System 1 &amp; System 2 thinking? And what can it teach us about the strengths, limitations and research avenues of AI research?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/north_star_phd_2/brockman-480.webp 480w,/assets/img/north_star_phd_2/brockman-800.webp 800w,/assets/img/north_star_phd_2/brockman-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/north_star_phd_2/brockman" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="system-1-and-system-2---a-paradigm-for-progress-in-ai-research">System 1 and System 2 - A Paradigm for Progress in AI Research</h2> <p>In his famous book “Thinking, Fast and Slow”, Daniel Kahneman popularized the concepts of <em>System 1</em> and <em>System 2</em> thinking. <em>System 1</em> refers to humans’ fast, intuitive, and automatic thought processes that requires little effort or conscious control. It’s what drives quick judgments, emotional reactions, instinctive decisions etc… Some typical examples of System 1 thinking are recognizing a friend’s face, understanding simple words (e.g. a STOP sign), or driving a car on an empty road. <em>System 2</em>, on the other hand, is our slower, more deliberate, and analytical thinking mode. It involves conscious reasoning, complex problem-solving, or long term planning. Examples of System 2 thinking include solving a complex math problem, choosing what flight you’ll take to arrive on vacation, or learning a new skill. Both System 1 and System 2 thinking continuously interact with one another.</p> <p>This has become a popular perspective of our intelligence, and often used by the Machine Learning community <d-footnote>NeurIPS now even has a "system 2 workshop": https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/Sys2-Reasoning </d-footnote>. Among the many things System 1 thinking is useful for, one thing of particular importance is the <em>filtering</em> through tools and memories to tackle a given situation. Imagine yourself unexpectedly being asked about a Calculus problem by a friend. Almost immediately, you are able to filter out the entirety of what you know from Algebra, Physics, Biology, and the rest of your knowledge space, and start thinking about a very very small subset of possible solutions to the problem. It then becomes a slower thinking process (system 2), but on a very small <strong>search space</strong> compared to the entirety of the toolbox that your brain possesses.</p> <p>Even more important than reducing the search space, the system 1 filtering allowed you to already have the right <strong>representation</strong> of the problem. You instinctively know that thinking about a mathematics problem is not the same as thinking about a music or cooking problem - the question that your friend asked you has a specific and adapted representation, and start your system 2 thinking process with the right “cognitive organization”. Often, you’ll realize that you have the wrong representation of the problem, and “need to think about it differently” - this is your System 2 communicating with your system 1.</p> <p>In effect, system 1 unconsciously molds your cognition in the right frame to solve daily problems an learn new skills. It just immediately extracts the right tools from your toolbox, without you ever realizing that it did. All you now have to do is think a bit whether to start working with the hammer or the nail - but never were you suggested to use a computer or a watch or a car to put that nail on the wall.</p> <h2 id="can-neural-networks-do-system-1">Can Neural Networks do System 1?</h2> <p>Yes, but they also are not so good at it. Let’s dive in.</p> <h3 id="neural-networks-are-quintessential-system-1">Neural Networks are quintessential System 1</h3> <p>Neural Networks are essentially “System 1 Thinkers” - in pretty much all their variations and forms. What fundamentally makes Neural Networks <em>System 1</em> thinkers can be summed up to a few points:</p> <ul> <li>The speed at which they produce a response is <em>irrespective of the complexity</em> of what they must process. It’s only constrained by the architectural and computational constrains of the neural network - just like “reflexes” in humans are constrained by the time constants of neural cells and other physiological factors. This is also analogous to the concept of “intuition” in humans - however complex a problem may be, we often have immediate intuitions over what the solution might be. This argument is central to the point - this immediate intuition allows to vastly reduce and guide the search space - it’s a pre-requisite to be able to solve the problem in reasonable time-span. System 2 thinking cannot function without a robust System 1 thinking. This argument is also central to the point in showing that System 1 thinking is not sufficient: currently, an LLM would take the same amount of time to compute “Hi, how are you doing?” as “26.9*124^7” - by virtue of it being the same number of input tokens.</li> <li>It is virtually impossible to obtain a “human interpretable” explanation to why NNs output response X as opposed to response Y, Z etc… All System 1 processes are indeed unexplained to humans - they just emerge as a function of architecture (native) and weights (trained).</li> <li>Neural Networks produce “<em>latent representation</em>” of stimuli and states. A good analogy in humans is the set of expectations one has when encountering a situation, as well as the attention on specific details of the situation. Only recognizing a person that comes talk to you at your desk creates a space of expectation and bounds your behavior in an immediate way. It also channels your attention on that interaction, rather than your other colleague typing on their computer or what’s going on outside the window. This is something I find particularly interesting that I also attribute to System 1 thinking. In symbolic terms - it’s a matter of shrinking the action space to search over in response to stimuli.</li> </ul> <h3 id="foundation-models-as-generalist-system-1-models">Foundation Models as generalist System 1 models</h3> <p><em>Foundation Models</em> are models that have been trained by large corporations/institutions on vast amount of training data. Some are open source, and some are not. Some focus on language, some on vision, and some on time series, and some on everything. Foundation Models can be considered the first general-purpose System 1 models. As Neural Networks, they perfectly fit the aforementioned categorization as System 1 models. What is novel and interesting in Foundation Models in that spirit, compared to the vast amount of NNs that have been trained and deployed in the past decades, is the diversity of their training data - they are unlike narrow NN which can only navigate specific problems for which they were trained for. They are trained on different kinds of input format, and most importantly, on a <em>very high</em> diversity of “topics”. This enables them to do <em>in-context learning</em> and <em>knowledge interpolation</em>. This makes them capable of dealing with new problem instances that are completely novel, in a way that has never been seen before - as long as the prompt resembles what these models have been trained on before. The reality is that the amount of training data they have been trained on is so vast that there are very little things that one might ask that have never been looked at by the model. Essentially, they are quite capable of having a good initial estimate of how to tackle pretty much any situation. Of course, we know countless papers that expose the failure modes of these models, but they always answer on topic. I believe this is analogous to us having to tackle daily situations without System 2 thinking - we’d be able to be on topic most of the times, but would much too frequently fail to have the right behavior or give the right answer to the problems we’re asked. It’s analogous to us only having intuition, but never slow deliberate thinking.</p> <p>Their training modality also enables them to quickly adapt to new categories of knowledge that one may want to fine tune them on. The great paper <a href="https://www.nature.com/articles/s41467-019-11786-6" rel="external nofollow noopener" target="_blank">A critique of Pure Learning</a> argued that biological brains learn very little - they are just incredibly well wired as a result millennia of genetic evolution, and adapt incredibly fast to new environments. One can think of Foundation models as such - models that have been trained on such vast amount of data that their architecture and weights are perfectly suited to learn new tasks quickly. This is what we see in practice - your average computer vision model is not trained from scratch anymore - one always loads a pre-trained model and fine tunes with their own (often-limited) data - and that usually does the job well <d-footnote>compared to training from scratch. Of course, these models still have significant issues.</d-footnote>.</p> <h3 id="whats-missing-for-good-system-1-thinking">What’s missing for good system 1 thinking?</h3> <p>In short: interaction and grounding.</p> <h4 id="interaction">Interaction</h4> <p>A fundamental aspect of our learning and intelligence, and of our developing intuitions about how things in the world work is through both <em>exposure</em> and <em>interaction</em> with these things. As I argue in my <a href="/_posts/2024-10-10-phd-north-star.md">previous post</a>, taking the number “3” as a demonstrating example: <em>“not only do we learn what “three” means by being shown three fingers, three pens, or three heads on Cerebrus’ head, but we also learn how to say three words, draw three lines, eat three meals a day. The concept “three” is not only seen but also <strong>interacted</strong> with, and even <strong>produced</strong> in a wide range of different contexts. Its meaning and representation in our brain is multi faceted and highly malleable - specifically because we have *actively</em> learned about it through interaction and production, as opposed to just <em>passively</em> through exposure.*. Learning from data is not enough.</p> <p>One research direction that tries to go beyond passively learning from data is the one of World Models. According to Claude, “World Models work by learning to create internal, predictive representations of an environment that allow an AI agent to simulate potential scenarios, forecast outcomes, and make more intelligent decisions by “dreaming” or exploring hypothetical trajectories before taking actual actions.”</p> <p>In the original World Model paper <d-footnote>arXiv:1803.10122v4</d-footnote>, Ha &amp; Schmidhuber write:</p> <blockquote> <p>“One way of understanding the predictive model inside of our brains is that it might not be about &gt;just predicting the future in general, but predicting future sensory data given our current motor &gt;actions.</p> </blockquote> <p>By attempting to predict future sensory input (data) based on current actions, one essentially gets this sense of interaction - one can see the direct effect of their actions on the world, and most importantly, accordingly correct their prediction errors in a more grounded way. Working with World Models come with plenty of other advantages (and limitations), but this will be for another blogpost. This research direction is a major one of my PhD.</p> <h4 id="grounding">Grounding</h4> <p>Symbolic grounding is a fundamental question in AI, which looks at how abstract symbolic representations (like words, concepts, or logical statements) acquire meaning and connection to real-world sensory experiences and perceptual data. Many make the argument that Neural Networks simply do not learn good symbolic representations by learning in the statistical way they do. However, some people argue otherwise. Ellie Pavlick dedicated an <a href="convincingly(https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0041)">entire (and beautiful) article</a> to outlining counterarguments to the common criticism of LLMs: 1) their lack of symbolic structure and 2) their lack of grounding. She points to research that, instead of showing failures of Neural Networks and using this as an argument against symbolic structure or grounding, looks at the representations that models use under the hood, which paints a much more positive picture.</p> <p>Long story short, I am now a bit unsure what to think about this.</p> <h2 id="can-neural-networks-do-system-2">Can Neural Networks do System 2?</h2> <h3 id="traditional-system-2---planners-and-search">Traditional System 2 - Planners And Search</h3> <h3 id="whats-missing">What’s Missing</h3> <h2 id="how-do-we-connect-system-1-and-system-2">How do we connect System 1 and System 2?</h2> <h3 id="chain-of-thought--test-time-training">Chain-Of-Thought &amp; Test-Time-Training</h3> <h3 id="neuro-symbolic-approaches">Neuro-Symbolic Approaches</h3> <h3 id="what-does-neuroscience-tell-us-about-system-1-and-system-2-thinking">What does Neuroscience tell us about System 1 and System 2 thinking?</h3> <p>&lt;!– - name: System 1 and System 2 - A Paradigm for Progress in AI Research</p> <ul> <li>name: Can Neural Networks do System 1?</li> <li>subsections: <ul> <li>name: Neural Networks are quintessential System 1</li> <li>name: Foundation Models as generalist System 1 models</li> <li>name: What’s missing for good system 1 thinking?</li> </ul> </li> <li>name: Can Neural Networks do System 2?</li> <li>subsections: <ul> <li>name: Traditional System 2 - Planners And Search</li> <li>name: What’s Missing</li> </ul> </li> <li>name: How do we connect System 1 and System 2?</li> <li>subsections: <ul> <li>name: Chain-Of-Thought &amp; Test-Time-Training</li> <li>name: Neuro-Symbolic Approaches</li> <li>name: What does Neuroscience tell us about System 1 and System 2 thinking? –&gt;</li> </ul> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yassine Taoudi-Benchekroun. Last updated: October 04, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>